import torch
import argparse
import torch.nn as nn
import torch.utils.data as Data
import torch.backends.cudnn as cudnn
from scipy.io import loadmat, savemat
from torch import optim
from vit_pytorch import ViT
from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score

import matplotlib.pyplot as plt
from matplotlib import colors
import numpy as np
import time
import os

# --- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì‹¤í–‰ ì˜µì…˜ ì„¤ì • ---
parser = argparse.ArgumentParser("HSI-Classification")
parser.add_argument('--dataset', choices=['Indian', 'Pavia', 'Houston'], default='Indian', help='ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì„ íƒ')
parser.add_argument('--mode', choices=['ViT', 'CAF'], default='ViT', help='ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„ íƒ (ViT: ê¸°ë³¸, CAF: SpectralFormer)')
parser.add_argument('--flag_test', choices=['test', 'train'], default='train', help='ì‹¤í–‰ ëª¨ë“œ (train ë˜ëŠ” test)')
parser.add_argument('--gpu_id', default='0', help='ì‚¬ìš©í•  GPU ID')
parser.add_argument('--seed', type=int, default=0, help='ìž¬í˜„ì„±ì„ ìœ„í•œ ëžœë¤ ì‹œë“œ')
parser.add_argument('--batch_size', type=int, default=64, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
parser.add_argument('--epoches', type=int, default=200, help='ì´ í›ˆë ¨ ì—í­ ìˆ˜')
parser.add_argument('--patches', type=int, default=7, help='ê³µê°„ íŒ¨ì¹˜ í¬ê¸° (e.g., 7 -> 7x7 íŒ¨ì¹˜)')
parser.add_argument('--band_patches', type=int, default=3, help='ê·¸ë£¹ë³„ ë¶„ê´‘ ìž„ë² ë”©(GSE)ì„ ìœ„í•œ ì¸ì ‘ ë°´ë“œ ìˆ˜ (í™€ìˆ˜ ê¶Œìž¥)')
parser.add_argument('--test_freq', type=int, default=10, help='ê²€ì¦ ì£¼ê¸° (ëª‡ ì—í­ë§ˆë‹¤ ê²€ì¦í• ì§€)')
parser.add_argument('--learning_rate', type=float, default=5e-4, help='í•™ìŠµë¥ ')
parser.add_argument('--gamma', type=float, default=0.9, help='í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ê°ë§ˆ ê°’')
parser.add_argument('--weight_decay', type=float, default=0, help='ê°€ì¤‘ì¹˜ ê°ì‡  (L2 ì •ê·œí™”)')
args = parser.parse_args()

# ëª¨ë¸ íŒŒì¼ ì´ë¦„ ë™ì  ìƒì„±
MODEL_FILENAME = f'./best_model_{args.dataset}_{args.mode}_patch{args.patches}.pt'

# GPU ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)

# ìž¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed(args.seed)
cudnn.deterministic = True
cudnn.benchmark = False

# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ ---

def chooose_train_and_test_point(train_data, test_data, true_data, num_classes):
    """í›ˆë ¨, í…ŒìŠ¤íŠ¸, ì „ì²´ ë°ì´í„°ì˜ ì¢Œí‘œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    pos_train, pos_test, pos_true = {}, {}, {}
    number_train, number_test, number_true = [], [], []

    for i in range(num_classes):
        pos_train[i] = np.argwhere(train_data == (i + 1))
        number_train.append(pos_train[i].shape[0])
        pos_test[i] = np.argwhere(test_data == (i + 1))
        number_test.append(pos_test[i].shape[0])

    total_pos_train = np.concatenate(list(pos_train.values()), axis=0).astype(int)
    total_pos_test = np.concatenate(list(pos_test.values()), axis=0).astype(int)

    for i in range(num_classes + 1): # 0ë²ˆ í´ëž˜ìŠ¤ (ë°°ê²½) í¬í•¨
        pos_true[i] = np.argwhere(true_data == i)
        number_true.append(pos_true[i].shape[0])

    total_pos_true = np.concatenate(list(pos_true.values()), axis=0).astype(int)

    return total_pos_train, total_pos_test, total_pos_true, number_train, number_test, number_true

def mirror_hsi(height, width, band, input_normalize, patch=7):
    """ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë¯¸ëŸ¬ë§í•˜ì—¬ ê³µê°„ íŒ¨ë”©ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜"""
    padding = patch // 2
    mirror_hsi = np.zeros((height + 2 * padding, width + 2 * padding, band), dtype=float)
    mirror_hsi[padding:(padding + height), padding:(padding + width), :] = input_normalize

    # ìƒí•˜ì¢Œìš° ê²½ê³„ ë¯¸ëŸ¬ë§
    mirror_hsi[0:padding, :, :] = mirror_hsi[padding:2*padding, :, :][::-1, :, :] # Top
    mirror_hsi[-padding:, :, :] = mirror_hsi[-2*padding:-padding, :, :][::-1, :, :] # Bottom
    mirror_hsi[:, 0:padding, :] = mirror_hsi[:, padding:2*padding, :][:, ::-1, :] # Left
    mirror_hsi[:, -padding:, :] = mirror_hsi[:, -2*padding:-padding, :][:, ::-1, :] # Right

    print("--- Data Preprocessing ---")
    print(f"Spatial patch size: {patch}x{patch}")
    print(f"Mirrored HSI shape: {mirror_hsi.shape}")
    return mirror_hsi

def create_dataset(mirror_image, points, band, patch_size, band_patch):
    """
    ì£¼ì–´ì§„ ì¢Œí‘œ(points)ì— ëŒ€í•´ ê³µê°„-ë¶„ê´‘ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬
    (ìƒ˜í”Œ ìˆ˜, ë°´ë“œ ìˆ˜, íŠ¹ì§• ë²¡í„° ì°¨ì›) í˜•íƒœì˜ ìµœì¢… ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    n_points = points.shape[0]
    print(f"\nCreating dataset for {n_points} samples...")
    
    # 1. ê³µê°„ íŒ¨ì¹˜ ì¶”ì¶œ: (ìƒ˜í”Œ ìˆ˜, íŒ¨ì¹˜, íŒ¨ì¹˜, ë°´ë“œ)
    spatial_patches = np.zeros((n_points, patch_size, patch_size, band), dtype=np.float32)
    for i, (r, c) in enumerate(points):
        spatial_patches[i, :, :, :] = mirror_image[r:(r + patch_size), c:(c + patch_size), :]
    
    # 2. ë°ì´í„° ì¶• ë³€í™˜ ë° ë¦¬ì‰ìžŽ: (ìƒ˜í”Œ ìˆ˜, ë°´ë“œ, ê³µê°„í”½ì…€)
    data_transposed = spatial_patches.transpose(0, 3, 1, 2).reshape(n_points, band, -1)

    # 3. ë¶„ê´‘ ì°¨ì› íŒ¨ë”© ('edge' ëª¨ë“œë¡œ ê²½ê³„ ë³µì œ)
    half_patch = band_patch // 2
    data_padded = np.pad(data_transposed, ((0, 0), (half_patch, half_patch), (0, 0)), 'edge')

    # 4. ë¶„ê´‘ íŠ¹ì§• ë²¡í„° ìƒì„±
    feature_dim = (patch_size**2) * band_patch
    final_features = np.zeros((n_points, band, feature_dim), dtype=np.float32)
    for i in range(band):
        spectral_window = data_padded[:, i:(i + band_patch), :]
        final_features[:, i, :] = spectral_window.reshape(n_points, -1)
        
    print(f"Final dataset shape: {final_features.shape} -> (Samples, Bands, Features)")
    return final_features

def create_labels(number_list, num_classes, include_background=False):
    """í´ëž˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ ë¼ë²¨ ë°°ì—´ì„ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    labels = []
    start_class = 0 if include_background else 1
    
    for i in range(len(number_list)):
        class_label = (i + start_class) if include_background else i
        labels.extend([class_label] * number_list[i])
        
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¼ë²¨ì´ 0ë¶€í„° ì‹œìž‘í•´ì•¼ í•¨
    if not include_background:
        return np.array(labels, dtype=np.int64)
    else: # ì „ì²´ ë°ì´í„°(true)ëŠ” ë°°ê²½(0)ì„ í¬í•¨
        return np.array(labels, dtype=np.int64)

def train_and_test_label(number_train, number_test, number_true, num_classes):
    """ë°ì´í„°ì— ë§žëŠ” ë¼ë²¨(y) ë°°ì—´ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    y_train = create_labels(number_train, num_classes)
    y_test = create_labels(number_test, num_classes)
    y_true = create_labels(number_true, num_classes, include_background=True)
    
    print("\n--- Label Shapes ---")
    print(f"y_train: shape = {y_train.shape}")
    print(f"y_test:  shape = {y_test.shape}")
    print(f"y_true:  shape = {y_true.shape}")
    print("--------------------")
    return y_train, y_test, y_true


# --- 3. í›ˆë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

class AverageMeter(object):
    """ê°’ë“¤ì˜ í‰ê· ê³¼ í•©ê³„ë¥¼ ê³„ì‚°í•˜ê³  ì €ìž¥í•˜ëŠ” í´ëž˜ìŠ¤"""
    def __init__(self): self.reset()
    def reset(self): self.avg, self.sum, self.cnt = 0, 0, 0
    def update(self, val, n=1):
        self.sum += val * n
        self.cnt += n
        self.avg = self.sum / self.cnt

def calculate_metrics(output, target):
    """ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ë° ê¸°íƒ€ ì •ë³´ë¥¼ ê³„ì‚°"""
    _, pred = output.topk(1, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    acc = correct[0].reshape(-1).float().sum(0) * 100.0 / target.size(0)
    return acc, target.cpu().numpy(), pred.squeeze().cpu().numpy()

def train_epoch(model, train_loader, criterion, optimizer):
    """1 ì—í­(epoch) ë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” í•¨ìˆ˜"""
    model.train()
    losses, accuracies = AverageMeter(), AverageMeter()

    for batch_data, batch_target in train_loader:
        batch_data, batch_target = batch_data.cuda(), batch_target.cuda()

        optimizer.zero_grad()
        batch_pred = model(batch_data)
        loss = criterion(batch_pred, batch_target)
        loss.backward()
        optimizer.step()

        acc, _, _ = calculate_metrics(batch_pred, batch_target)
        losses.update(loss.item(), batch_data.size(0))
        accuracies.update(acc.item(), batch_data.size(0))

    return accuracies.avg, losses.avg

def valid_epoch(model, valid_loader):
    """ëª¨ë¸ì„ ê²€ì¦(validation)í•˜ëŠ” í•¨ìˆ˜ (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì—†ìŒ)"""
    model.eval()
    all_targets, all_preds = np.array([]), np.array([])

    with torch.no_grad():
        for batch_data, batch_target in valid_loader:
            batch_data, batch_target = batch_data.cuda(), batch_target.cuda()
            batch_pred = model(batch_data)
            _, t, p = calculate_metrics(batch_pred, batch_target)
            all_targets = np.append(all_targets, t)
            all_preds = np.append(all_preds, p)

    return all_targets, all_preds

def test_and_visualize(model, test_loader, true_points, y_true, height, width, color_map, ground_truth_map):
    """ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ë¶„ë¥˜ ì§€ë„ë¥¼ ìƒì„± ë° ì‹œê°í™”"""
    model.eval()
    all_preds = np.array([])
    with torch.no_grad():
        for batch_data, _ in test_loader:
            batch_data = batch_data.cuda()
            batch_pred = model(batch_data)
            _, pred = batch_pred.topk(1, 1, True, True)
            all_preds = np.append(all_preds, pred.squeeze().cpu().numpy())
    
    # ì „ì²´ ë§µ ìƒì„±
    prediction_map = np.zeros((height, width))
    for i, (r, c) in enumerate(true_points):
        if y_true[i] != 0: # ë°°ê²½(ë¼ë²¨ 0)ì´ ì•„ë‹Œ í”½ì…€ë§Œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì±„ì›€
            prediction_map[r, c] = all_preds[i] + 1
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    ax1.imshow(prediction_map, cmap=colors.ListedColormap(color_map))
    ax1.set_title(f'{args.dataset} Classification Map ({args.mode})')
    ax1.axis('off')
    
    ax2.imshow(ground_truth_map, cmap=colors.ListedColormap(color_map))
    ax2.set_title('Ground Truth')
    ax2.axis('off')
    
    plt.show()
    
    # .mat íŒŒì¼ë¡œ ì €ìž¥
    save_filename = f'prediction_{args.dataset}_{args.mode}_patch{args.patches}.mat'
    savemat(save_filename, {'prediction': prediction_map, 'ground_truth': ground_truth_map})
    print(f"Prediction map saved to {save_filename}")

def output_metric(tar, pre):
    """ì„±ëŠ¥ ì§€í‘œ(OA, AA, Kappa)ë¥¼ ê³„ì‚°í•˜ê³  ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
    matrix = confusion_matrix(tar, pre)
    class_accuracy = np.diag(matrix) / matrix.sum(axis=1)
    AA = np.mean(class_accuracy) # Average Accuracy
    OA = accuracy_score(tar, pre) # Overall Accuracy
    Kappa = cohen_kappa_score(tar, pre)
    return OA, AA, Kappa, class_accuracy

# --- 4. ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---

# ë°ì´í„°ì…‹ ë¡œë“œ
print(f"Loading {args.dataset} dataset...")
if args.dataset == 'Indian':
    data = loadmat('./data/IndianPine.mat')
elif args.dataset == 'Pavia':
    data = loadmat('./data/Pavia.mat')
elif args.dataset == 'Houston':
    data = loadmat('./data/Houston.mat')
else:
    raise ValueError("Unknown dataset")

color_mat = loadmat('./data/AVIRIS_colormap.mat')
TR = data['TR']
TE = data['TE']
input_data = data['input']
label = TR + TE
num_classes = np.max(TR)
color_matrix = color_mat[list(color_mat.keys())[-1]]

# ë°ì´í„° ì •ê·œí™” (0~1)
input_normalize = np.zeros_like(input_data, dtype=float)
for i in range(input_data.shape[2]):
    input_max, input_min = np.max(input_data[:, :, i]), np.min(input_data[:, :, i])
    input_normalize[:, :, i] = (input_data[:, :, i] - input_min) / (input_max - input_min)

height, width, band = input_data.shape
print(f"HSI data shape: height={height}, width={width}, bands={band}")

# ë°ì´í„° ì¢Œí‘œ ë° ë¼ë²¨ ìƒì„±
total_pos_train, total_pos_test, total_pos_true, num_train, num_test, num_true = chooose_train_and_test_point(TR, TE, label, num_classes)
y_train, y_test, y_true = train_and_test_label(num_train, num_test, num_true, num_classes)

# íŒ¨ì¹˜ ë°ì´í„°ì…‹ ìƒì„±
mirror_image = mirror_hsi(height, width, band, input_normalize, patch=args.patches)
x_train = create_dataset(mirror_image, total_pos_train, band, args.patches, args.band_patches)
x_test = create_dataset(mirror_image, total_pos_test, band, args.patches, args.band_patches)
x_true = create_dataset(mirror_image, total_pos_true, band, args.patches, args.band_patches)

# PyTorch DataLoader ìƒì„±
train_loader = Data.DataLoader(Data.TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train).long()), batch_size=args.batch_size, shuffle=True)
test_loader = Data.DataLoader(Data.TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test).long()), batch_size=args.batch_size, shuffle=False)
true_loader = Data.DataLoader(Data.TensorDataset(torch.from_numpy(x_true), torch.from_numpy(y_true).long()), batch_size=256, shuffle=False)

# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜
model = ViT(
    image_size=args.patches,
    near_band=args.band_patches,
    num_patches=band,
    num_classes=num_classes,
    dim=64, depth=5, heads=4, mlp_dim=8,
    dropout=0.1, emb_dropout=0.1,
    mode=args.mode
).cuda()

criterion = nn.CrossEntropyLoss().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.epoches // 10, gamma=args.gamma)

# --- 5. ë©”ì¸ ë£¨í”„: í›ˆë ¨ ë˜ëŠ” í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---
if args.flag_test == 'train':
    print("\n--- Start Training ---")
    tic = time.time()
    best_oa = 0

    for epoch in range(args.epoches):
        train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer)
        scheduler.step()

        print(f"Epoch: {epoch + 1:03d}/{args.epoches:03d} | "
              f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")

        if (epoch + 1) % args.test_freq == 0 or (epoch + 1) == args.epoches:
            tar_v, pre_v = valid_epoch(model, test_loader)
            oa, aa, kappa, _ = output_metric(tar_v, pre_v)
            print(f"  => Validation | OA: {oa:.4f} | AA: {aa:.4f} | Kappa: {kappa:.4f}")

            if oa > best_oa:
                best_oa = oa
                torch.save(model.state_dict(), MODEL_FILENAME)
                print(f"  => ðŸŽ‰ New best model saved to {MODEL_FILENAME} with OA: {best_oa:.4f}")
    
    toc = time.time()
    print(f"\n--- Training Finished in {toc - tic:.2f}s ---")
    print("\n--- Final Evaluation with Best Model ---")
    args.flag_test = 'test' # í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜

if args.flag_test == 'test':
    if not os.path.exists(MODEL_FILENAME):
        raise FileNotFoundError(f"Model file not found at {MODEL_FILENAME}. Please train the model first.")
        
    model.load_state_dict(torch.load(MODEL_FILENAME))
    print(f"Model loaded from {MODEL_FILENAME}")

    final_tar, final_pre = valid_epoch(model, test_loader)
    OA, AA, Kappa, class_acc = output_metric(final_tar, final_pre)
    print(f"\n--- Final Test Metrics ---")
    print(f"Overall Accuracy (OA): {OA:.4f}")
    print(f"Average Accuracy (AA): {AA:.4f}")
    print(f"Kappa Coefficient:     {Kappa:.4f}")
    print("\nClass-wise Accuracies:")
    for i, acc in enumerate(class_acc):
        print(f"  Class {i+1:2d}: {acc:.4f}")
        
    test_and_visualize(model, true_loader, total_pos_true, y_true, height, width, color_matrix, label)

print("\n--- Parameters Used ---")
for k, v in vars(args).items():
    print(f"{k}: {v}")
print("-----------------------")
